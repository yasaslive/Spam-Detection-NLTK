{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10054]\n",
      "[nltk_data]     An existing connection was forcibly closed by the\n",
      "[nltk_data]     remote host>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dobmeos with hgh my energy level has gone up !...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>your prescription is ready . . oxwq s f e low ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get that new car 8434 people nowthe weather or...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>await your response dear partner , we are a te...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coca cola , mbna america , nascar partner with...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Class\n",
       "0  dobmeos with hgh my energy level has gone up !...  spam\n",
       "1  your prescription is ready . . oxwq s f e low ...  spam\n",
       "2  get that new car 8434 people nowthe weather or...  spam\n",
       "3  await your response dear partner , we are a te...  spam\n",
       "4  coca cola , mbna america , nascar partner with...  spam"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assignment 2 - Building text processing models\n",
    "\n",
    "# Group Details\n",
    "# 01. IT17181402 D. G. Y. C. K. Alwis\n",
    "# 02. IT17182188 R. M. M. G. Rathnayake\n",
    "# 03. IT17175012 W. M. N. A. Fernando\n",
    "\n",
    "#                 Topic               #\n",
    "#         Spam Identification\n",
    "\n",
    "# Dataset used in the assignment\n",
    "# Kaggle SMS Spam Collection Dataset - Visit: https://github.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset- \n",
    "\n",
    "#             Introduction            #\n",
    "#  Spam is a irrelevant or unsolicited messages sent over the Internet, \n",
    "#  typically to a large number of users, for the purposes of advertising, phishing, spreading malware.\n",
    "#  To avoid this unnessary sms or emails we can build a model that identify the spams using specific keywords such as WIN, Bouns, Winner etc.\n",
    "#  The code is such spam detection model that implement using python. enjoy!\n",
    "\n",
    "# Spam vs Ham \n",
    "# \"Ham\" is e-mail that is not Spam. In other words, \"non-spam\", or \"good mail\". It should be considered a shorter, \n",
    "# snappier synonym for \"non-spam\". Its usage is particularly common among anti-spam software developers, \n",
    "# and not widely known elsewhere; in general it is probably better to use the term \"non-spam\", instead.\n",
    "\n",
    "#Loading nessasary libraries\n",
    "\n",
    "import nltk #importing natural language processing toolkit visit: https://www.nltk.org/ \n",
    "import numpy as np #importing numpy or python array object library visit: https://numpy.org/\n",
    "import pandas as pd #importing pandas libarary, pyton data analysis library visit: https://pandas.pydata.org/\n",
    "import matplotlib.pyplot as plt #importing matplotlib library, python data visualization library visit: https://matplotlib.org/\n",
    "from sklearn.feature_extraction.text import CountVectorizer #importing count vectorizer visit: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import TfidfTransformer #importing tf-idf calculator visit: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier #importing classification algorithm visit: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "from sklearn.model_selection import train_test_split #importing data splitter visit: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.metrics import accuracy_score #importing accuracy score to determine model accuracy visit: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "from sklearn.metrics import confusion_matrix #importing confution matrix visit: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "from sklearn.metrics import classification_report # importing classification report to print precision, recall and f1 visit: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report \n",
    "from nltk.tokenize import word_tokenize #importing word tokenizer visit: https://www.nltk.org/api/nltk.tokenize.html\n",
    "from nltk.corpus import stopwords #importing stopwords to remove stop words visit: https://pythonspot.com/nltk-stop-words/\n",
    "from nltk.stem.wordnet import WordNetLemmatizer #importing word lemmatizer visit: https://www.nltk.org/_modules/nltk/stem/wordnet.html\n",
    "from nltk.stem.porter import PorterStemmer #importing porterstemmer for stemming visit:  https://www.nltk.org/_modules/nltk/stem/porter.html\n",
    "import string #importing string library to manipulate strings visit: https://docs.python.org/2/library/string.html\n",
    "\n",
    "#download the nsessary stop word, punktuation, lemmatization libraries, \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#importing dataset using pandas dataframe\n",
    "dataset = pd.read_csv(\"dataset.csv\") #Please change the data file path when executing!\n",
    "\n",
    "#print header or first 5 rows of the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [dobmeo, hgh, energi, level, gone, stukm, intr...\n",
       "1    [prescript, readi, oxwq, f, low, cost, prescri...\n",
       "2    [get, new, car, peopl, nowth, weather, climat,...\n",
       "3    [await, respons, dear, partner, team, govern, ...\n",
       "4    [coca, cola, mbna, america, nascar, partner, o...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the message part to clean process\n",
    "\n",
    "#import stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#remove all NaN ot NaT (Null) Values visit: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "#Extarct the message from the dataset to process futher\n",
    "message = dataset['Text']\n",
    "message.head()\n",
    "\n",
    "#turn all the letters in the dataframe to lowercase visit: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.lower.html\n",
    "message = message.str.lower()\n",
    "\n",
    "#remove any html tags in the dataset visit: https://www.w3schools.com/python/python_regex.asp\n",
    "message = message.str.replace(r'<[^>]+>',' ')\n",
    "\n",
    "#remove all special characters visit: https://www.w3schools.com/python/python_regex.asp\n",
    "message = message.str.replace(r'[!\"#$%&\\'()*+,./:;<=>?@\\^_`{|}~-]',' ')\n",
    "\n",
    "#remove all numbers\n",
    "message = message.str.replace(r'[0-9]',' ')\n",
    "\n",
    "#remove single characters in the dataset\n",
    "message = message.str.replace(r'[ ][a-z][ ]',' ')\n",
    "\n",
    "#remove multiple spaces visit : https://www.w3schools.com/python/python_regex.asp\n",
    "message = message.str.replace(r'[ ]{2,}', ' ')\n",
    "\n",
    "#apply tokenization to the dataframe visit: https://www.nltk.org/api/nltk.tokenize.html\n",
    "message = message.apply(nltk.word_tokenize)\n",
    "\n",
    "#remove stop words from the dataframe\n",
    "message = message.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#lemmatize the words \n",
    "message = message.apply(lambda x:[lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "#setmming the words\n",
    "message = message.apply(lambda x:[stemmer.stem(word) for word in x])\n",
    "\n",
    "#print five rows of the processed data\n",
    "message.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    dobmeo hgh energi level gone stukm introduc do...\n",
       "1    prescript readi oxwq f low cost prescript medi...\n",
       "2    get new car peopl nowth weather climat particu...\n",
       "3    await respons dear partner team govern offici ...\n",
       "4    coca cola mbna america nascar partner otcbb im...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From the cleaning process we get dataframe Series of lists\n",
    "#Remove list and create the sentence again to calculate tf-idf \n",
    "build_messages = message.str.join(' ')\n",
    "build_messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 47011)\t1\n",
      "  (0, 48924)\t1\n",
      "  (0, 37405)\t1\n",
      "  (0, 38177)\t1\n",
      "  (0, 9001)\t1\n",
      "  (0, 19561)\t1\n",
      "  (0, 30929)\t1\n",
      "  (0, 43717)\t1\n",
      "  (0, 29682)\t1\n",
      "  (0, 8063)\t1\n",
      "  (0, 9640)\t2\n",
      "  (0, 20045)\t1\n",
      "  (0, 43057)\t1\n",
      "  (0, 21761)\t1\n",
      "  (0, 9124)\t1\n",
      "  (0, 38133)\t1\n",
      "  (0, 35313)\t1\n",
      "  (0, 40593)\t1\n",
      "  (0, 1184)\t1\n",
      "  (0, 28624)\t1\n",
      "  (0, 28590)\t1\n",
      "  (0, 42550)\t1\n",
      "  (0, 14415)\t1\n",
      "  (0, 41497)\t1\n",
      "  (0, 44626)\t1\n",
      "  :\t:\n",
      "  (4, 37511)\t1\n",
      "  (4, 40724)\t2\n",
      "  (4, 49508)\t1\n",
      "  (4, 40396)\t1\n",
      "  (4, 28079)\t2\n",
      "  (4, 728)\t1\n",
      "  (4, 40534)\t1\n",
      "  (4, 9287)\t1\n",
      "  (4, 871)\t3\n",
      "  (4, 22734)\t5\n",
      "  (4, 22764)\t1\n",
      "  (4, 14027)\t1\n",
      "  (4, 32094)\t1\n",
      "  (4, 33517)\t1\n",
      "  (4, 6899)\t3\n",
      "  (4, 18189)\t1\n",
      "  (4, 34801)\t1\n",
      "  (4, 40232)\t1\n",
      "  (4, 26229)\t3\n",
      "  (4, 37405)\t1\n",
      "  (4, 30929)\t2\n",
      "  (4, 26863)\t2\n",
      "  (4, 45101)\t1\n",
      "  (4, 4087)\t1\n",
      "  (4, 32389)\t1\n",
      "convict\n"
     ]
    }
   ],
   "source": [
    "#Count vectorizer convert a collection of text documents to a matrix of token counts\n",
    "countvc = CountVectorizer() #https://adataanalyst.com/scikit-learn/countvectorizer-sklearn-example/\n",
    "\n",
    "#Fit data to the counter vectorizer and get document-term matrix\n",
    "document_term_matrix = countvc.fit_transform(stri for stri in build_messages)\n",
    "\n",
    "#print the document-term matrix\n",
    "print(document_term_matrix[0:5,:])\n",
    "\n",
    "#get the relevent term\n",
    "print(countvc.get_feature_names()[9667])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 50818)\t0.06696254378696476\n",
      "  (0, 49894)\t0.10055967195469231\n",
      "  (0, 49816)\t0.09869802882262106\n",
      "  (0, 48924)\t0.053503236914028425\n",
      "  (0, 48129)\t0.08227713600876241\n",
      "  (0, 47011)\t0.06273742979770587\n",
      "  (0, 46241)\t0.08384013051884252\n",
      "  (0, 45101)\t0.0319410785723564\n",
      "  (0, 44844)\t0.10397284981319102\n",
      "  (0, 44626)\t0.11963570438029639\n",
      "  (0, 43717)\t0.06891761144458002\n",
      "  (0, 43163)\t0.12491052537086636\n",
      "  (0, 43057)\t0.08968066277938404\n",
      "  (0, 43056)\t0.08044826193500586\n",
      "  (0, 42550)\t0.08968066277938404\n",
      "  (0, 41497)\t0.08110433250671219\n",
      "  (0, 41396)\t0.09057821238215277\n",
      "  (0, 40593)\t0.07675914327887542\n",
      "  (0, 39980)\t0.07496190279948364\n",
      "  (0, 38177)\t0.08968066277938404\n",
      "  (0, 38133)\t0.0995955784253898\n",
      "  (0, 37686)\t0.05753569500651184\n",
      "  (0, 37655)\t0.06566773249933254\n",
      "  (0, 37405)\t0.05353014345779247\n",
      "  (0, 36906)\t0.08884106119791826\n",
      "  :\t:\n",
      "  (10879, 11095)\t0.06171564628653134\n",
      "  (10879, 10303)\t0.04054969603488761\n",
      "  (10879, 9640)\t0.06865725688332348\n",
      "  (10879, 9600)\t0.030724673170547778\n",
      "  (10879, 9571)\t0.02373396747607812\n",
      "  (10879, 9032)\t0.040926725063561126\n",
      "  (10879, 9025)\t0.05399863475774593\n",
      "  (10879, 8579)\t0.039597340413046966\n",
      "  (10879, 8155)\t0.06569700174099011\n",
      "  (10879, 7693)\t0.024337617251618475\n",
      "  (10879, 7503)\t0.04014607035099382\n",
      "  (10879, 6986)\t0.03878458676365155\n",
      "  (10879, 6844)\t0.1680481556690742\n",
      "  (10879, 3013)\t0.5660130176702267\n",
      "  (10879, 2897)\t0.021974798606580886\n",
      "  (10879, 2675)\t0.02756052847624033\n",
      "  (10879, 1918)\t0.03491338801861991\n",
      "  (10879, 1719)\t0.06899481301438269\n",
      "  (10879, 1398)\t0.031501117016598267\n",
      "  (10879, 852)\t0.03818306892408666\n",
      "  (10879, 450)\t0.02706427647616336\n",
      "  (10879, 439)\t0.03525806978916975\n",
      "  (10879, 417)\t0.03471360075478563\n",
      "  (10879, 248)\t0.05996376922035077\n",
      "  (10879, 102)\t0.07727772165588939\n",
      "9.601626247391067\n"
     ]
    }
   ],
   "source": [
    "#get the tf-idf from the document-term matrix\n",
    "\n",
    "#create new object from tf-idf class\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "#Input the document-term matrix and transfrom it to tf-idf\n",
    "tfidf = tfidf_transformer.fit_transform(document_term_matrix)\n",
    "\n",
    "#print tf-idf vales in the dataset\n",
    "print(tfidf)\n",
    "\n",
    "#get tf-idf value for word control\n",
    "print (tfidf_transformer.idf_[countvc.vocabulary_['bitambr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devide dataset into train set and test set\n",
    "tfidf_trainset, tfidf_testset = train_test_split(tfidf,train_size=0.8)\n",
    "tfidf_train_class, tfidf_test_class = train_test_split(dataset['Class'],train_size=0.8)\n",
    "\n",
    "#Extract y or predicted ouput\n",
    "expected_output = tfidf_test_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Naive Bayes model: \n",
      "0.7320772058823529\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.74      0.84      2139\n",
      "        spam       0.02      0.35      0.04        37\n",
      "\n",
      "    accuracy                           0.73      2176\n",
      "   macro avg       0.50      0.55      0.44      2176\n",
      "weighted avg       0.97      0.73      0.83      2176\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[1580  559]\n",
      " [  24   13]]\n"
     ]
    }
   ],
   "source": [
    "#Creating Naive Bayes Model\n",
    "NV_model = MultinomialNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "NV_model.fit(tfidf_trainset, tfidf_train_class)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "test_prediction = NV_model.predict(tfidf_testset)\n",
    "\n",
    "print('Accuracy of the Naive Bayes model: ')\n",
    "print(NV_model.score(tfidf_testset,tfidf_test_class))\n",
    "\n",
    "\n",
    "print('\\nClassification Report')\n",
    "print(classification_report(test_prediction, expected_output))\n",
    "\n",
    "print('\\nConfusion Matrix')\n",
    "print(confusion_matrix(test_prediction, expected_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:  0.7297794117647058\n",
      "\n",
      "Confusion Matrix:  [[1582  566]\n",
      " [  22    6]]\n",
      "\n",
      "Report:                precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.74      0.84      2148\n",
      "        spam       0.01      0.21      0.02        28\n",
      "\n",
      "    accuracy                           0.73      2176\n",
      "   macro avg       0.50      0.48      0.43      2176\n",
      "weighted avg       0.97      0.73      0.83      2176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Building Support Vector Machine \n",
    "svm_model = SVC(kernel = 'rbf', gamma = 'scale', C = 1)\n",
    "\n",
    "#Fit data to the model to train\n",
    "svm_model.fit(tfidf_trainset,tfidf_train_class)\n",
    "\n",
    "#Predict the model using test dataset \n",
    "svm_predicted_output = svm_model.predict(tfidf_testset)\n",
    "\n",
    "print('Accuracy of the model: ',svm_model.score(tfidf_testset, expected_output))\n",
    "print('\\nConfusion Matrix: ',confusion_matrix(svm_predicted_output, expected_output))\n",
    "print('\\nReport: ', classification_report(svm_predicted_output, expected_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:  0.7247242647058824\n",
      "\n",
      "Confusion Matrix:  [[1569  564]\n",
      " [  35    8]]\n",
      "\n",
      "Report:                precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.74      0.84      2133\n",
      "        spam       0.01      0.19      0.03        43\n",
      "\n",
      "    accuracy                           0.72      2176\n",
      "   macro avg       0.50      0.46      0.43      2176\n",
      "weighted avg       0.96      0.72      0.82      2176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Building K-NN model \n",
    "\n",
    "#Creating K-NN model and set initial neighbors to 5\n",
    "#After running several times with random k value we found that the 13 is most accurate\n",
    "model = KNeighborsClassifier(n_neighbors=13)\n",
    "\n",
    "#fit the data into model train\n",
    "model.fit(tfidf_trainset,tfidf_train_class)\n",
    "\n",
    "kn_predicted_output = model.predict(tfidf_testset)\n",
    "\n",
    "print('Accuracy of the model: ',model.score(tfidf_testset,tfidf_test_class))\n",
    "print('\\nConfusion Matrix: ',confusion_matrix(kn_predicted_output,tfidf_test_class))\n",
    "print('\\nReport: ', classification_report(kn_predicted_output,tfidf_test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
